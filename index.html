<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/blogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blogs/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blogs/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhuogege1943.com","root":"/blogs/","images":"/blogs/images","scheme":"Muse","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/blogs/js/config.js"></script>

    <meta name="description" content="Hi, nice to meet you">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhuo&#39;s Blog">
<meta property="og:url" content="https://zhuogege1943.com/blogs/index.html">
<meta property="og:site_name" content="Zhuo&#39;s Blog">
<meta property="og:description" content="Hi, nice to meet you">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhuo ge ge">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zhuogege1943.com/blogs/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhuo's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/blogs/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/blogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhuo's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhuo ge ge</p>
  <div class="site-description" itemprop="description">Hi, nice to meet you</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blogs/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2024/12/27/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blogs/2024/12/27/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-27 23:07:06" itemprop="dateCreated datePublished" datetime="2024-12-27T23:07:06+02:00">2024-12-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2024/12/26/Megatron-LM-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blogs/2024/12/26/Megatron-LM-2/" class="post-title-link" itemprop="url">Megatron-LM (2)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-26 10:22:32" itemprop="dateCreated datePublished" datetime="2024-12-26T10:22:32+02:00">2024-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-27 16:30:49" itemprop="dateModified" datetime="2024-12-27T16:30:49+02:00">2024-12-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blogs/categories/Paper-reading/" itemprop="url" rel="index"><span itemprop="name">Paper reading</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Efficient-Large-Scale-Language-Model-Training-on-GPU-Clusters-Using-Megatron-LM"><a href="#Efficient-Large-Scale-Language-Model-Training-on-GPU-Clusters-Using-Megatron-LM" class="headerlink" title="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"></a>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h2><p>paper (2021 arxiv): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04473">https://arxiv.org/abs/2104.04473</a></p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>Based on Megatron-LM, the authors further adopt pipeline model parallelism via the proposed interleaved 1F1B pipeline schedules to scale the LLM training to thousands of GPUs, because of the dramatic increase in model sizes.<br><img src="/blogs/joplin_resources/5bc748b36099a88976c17f640972d76b.png"></p>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p><img src="/blogs/joplin_resources/98465e80b56627ea161b155d620a0b07.png"></p>
<p>Basically, it combines tensor parallelism (proposed in Megatron-LM) and pipeline parallelism for model parallelism. Assume tensor-parallel size is $t$, pipeline-parallel size is $p$ (also said as the number of pipeline stages), and data-parallel size is $d$, then the total number of GPUs is $ptd$.</p>
<p>First, let&#39;s get familiar with some concepts:</p>
<h4 id="GPipe-pipeline-schedule"><a href="#GPipe-pipeline-schedule" class="headerlink" title="GPipe pipeline schedule"></a>GPipe pipeline schedule</h4><p><img src="/blogs/joplin_resources/6575e9290eb6cf6aa228877c5239e812.png"></p>
<blockquote>
<p>The grey area represents <span style="color: red;">pipeline bubble</span> where devices are idle. <span style="color: red;">Pipeline flush</span>, I guess, is the end point of the backward pass at each iteration.</p>
</blockquote>
<p>Here, the number of microbatches in batch is $m$, and assume the time to execute a single microbatch’s forward and backward pass as $t_f$ and $t_b$. Then the total amount of time spent in the pipeline bubble is $t_{pb} &#x3D; (p-1)\cdot (t_f + t_b)$, and the ideal processing time for the batch is $t_{id} &#x3D; m\cdot (t_f + t_b)$. Therefore, the fraction of ideal computation time spent in the pipeline bubble (or called the <span style="color: red;">bubble time fraction</span>) is:</p>
<p>$$<br>\frac{t_{pb}}{t_{id}} &#x3D; \frac{p - 1}{m}.<br>$$</p>
<p>The bubble time fraction should be as small as possible. The naive solution is to make $m \gg p$, however, this needs each device to store all the m microbatches’ activations for the gradient calculation in the backward pass, having a high memory footprint. In other words, the number of in-flight microbatches equal to the total number of microbatches $m$, so we have:</p>
<h4 id="PipeDream-Flush-schedule-upper-part-of-the-following-figure"><a href="#PipeDream-Flush-schedule-upper-part-of-the-following-figure" class="headerlink" title="PipeDream-Flush schedule (upper part of the following figure)"></a>PipeDream-Flush schedule (upper part of the following figure)</h4><p><img src="/blogs/joplin_resources/6e4ffba59ca4f57b0bbc132fd516ee3b.png"></p>
<p>In this schedule, there is a warmup phase for each device for the forward pass. After the warmup, the device goes with a one forward pass followed by one backward pass, which is the so-called <span style="color: red;">1F1B</span> schedule. In this way, the number of in-flight microbatches reduces to $p$ in maximum, while the bubble fraction time is the same. Therefore, PipeDream-Flush can be much more memory-efficient when $p \ll m$. </p>
<p>To reduce the bubble fraction time and keep the schedule memory efficient, the authors propose:</p>
<h4 id="Interleaved-1F1B-pipeline-schedule-lower-part-of-the-above-figure"><a href="#Interleaved-1F1B-pipeline-schedule-lower-part-of-the-above-figure" class="headerlink" title="Interleaved 1F1B pipeline schedule (lower part of the above figure)"></a>Interleaved 1F1B pipeline schedule (lower part of the above figure)</h4><p>Briefly, each device can perform computation for multiple subsets of layers (or a <span style="color: red;">model chunk</span>), for example, device 1 has layers 1, 2, 9, 10, device 2 has layers 3, 4, 11, 12, and so on. Then just like the 1F1B schedule, they do an interleaved 1F1B schedule. </p>
<ul>
<li>Property 1: this needs the number of microbatches $m$ to be an integer multiple of $p$;</li>
<li>Property 2: this reduce the pipeline bubble time to $(p-1)\cdot(t_f+t_b)&#x2F;v$ where $v$ is the number of model chunks in each stage. Then the bubble time fraction reduces to $(p-1)&#x2F;(m\cdot v)$.</li>
<li>Property 3 (drawback): this introduces extra communication with the increase of $v$.</li>
</ul>
<h3 id="Higher-level-experimental-and-analytical-conclusion"><a href="#Higher-level-experimental-and-analytical-conclusion" class="headerlink" title="Higher-level experimental and analytical conclusion"></a>Higher-level experimental and analytical conclusion</h3><p>The actual throughput for each device is affected by all the hyperparameters $p, t, d, B, b$ etc., where $B$ is the global batch size and $b$ is the microbatch size due to the communication overhead between devices. There are three takeaways</p>
<ol>
<li><p>When using $g$-GPU servers, the tensor model parallelism should generally be set up to $g$. Based on that, pipeline model parallelism can be used across servers.</p>
</li>
<li><p>When combine data and model parallelism, for model parallelism, a total number of $t\cdot p$ GPUs should be used to fit the model memory, then data parallelism is used to scale up training.</p>
</li>
<li><p>The optimal $b$ depends on the characteristics and throughput of the model, $p$, $d$ and $B$.</p>
</li>
</ol>
<h3 id="Communication-optimization"><a href="#Communication-optimization" class="headerlink" title="Communication optimization"></a>Communication optimization</h3><p><img src="/blogs/joplin_resources/8650bd09481bc21b74e8e18f303e065f.png"></p>
<p>Simply, shown in the above figure, assume we have $t&#x3D;2$, when send and receive between two consecutive pipeline stage (i.e., two servers according to conclusion 1), the naive way is to send the tensor on each GPU on the previous pipeline stage (server) to the second stage, where each pair of GPUs on the sender and receiver communicate with the exact same set of tensor.  Instead, we can first divide (scatter) the tensor to be sent into $t$ parts equally and each GPU on the sender server send its part to the GPU on the receiver server, then use all-gather operator to gather the tensor. We call this scatter&#x2F;gather optimization that reduce the communication to $1&#x2F;t$. </p>
<h3 id="General-accelerator-agnostic-ideas"><a href="#General-accelerator-agnostic-ideas" class="headerlink" title="General accelerator-agnostic ideas"></a>General accelerator-agnostic ideas</h3><ol>
<li>Smartly partitioning the model training graph to minimize the amount of communication while still keeping device active (they are saying the interleaved 1F1B pipeline schedule).</li>
<li>Minimizing the number of memory bound kernels with operator fusion and careful data layout (they might be saying the fusion of output embedding with the cross entropy loss function to reduce the communication overhead).</li>
<li>Other domain-specific optimizations (like the scatter-gather optimization).</li>
</ol>
<h3 id="Experiment-results"><a href="#Experiment-results" class="headerlink" title="Experiment results"></a>Experiment results</h3><p><img src="/blogs/joplin_resources/e6acb1a769d391f7aaa6b4d449df3a7f.png"></p>
<p>  <img src="/blogs/joplin_resources/c63f7301c8ccbcbdd2535c492b8ecd36.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2024/12/26/Megatron-LM-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blogs/2024/12/26/Megatron-LM-1/" class="post-title-link" itemprop="url">Megatron-LM (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-26 08:54:32" itemprop="dateCreated datePublished" datetime="2024-12-26T08:54:32+02:00">2024-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-27 15:50:11" itemprop="dateModified" datetime="2024-12-27T15:50:11+02:00">2024-12-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blogs/categories/Paper-reading/" itemprop="url" rel="index"><span itemprop="name">Paper reading</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>Resources about distributed training with Megatron-LM</p>
</blockquote>
<p>Github: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a><br>Document on NeMo: <a target="_blank" rel="noopener" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html">https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html</a></p>
<blockquote>
<blockquote>
<p>NeMo is a cloud-native generative AI framework built on top of Megatron-LM.</p>
</blockquote>
</blockquote>
<p>Overall view of Megatron-Core: <a target="_blank" rel="noopener" href="https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html">https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html</a></p>
<blockquote>
<blockquote>
<p>Official APIs with formal product support.. </p>
</blockquote>
</blockquote>
<p>Megatron-LM are basically based on the following three papers. Let’s do some notes on them. </p>
<h2 id="Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism"><a href="#Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism" class="headerlink" title="Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"></a>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</h2><p>paper (2020, arxiv): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ol>
<li>Put large transformer models into different GPUs (with <code>tensor model parallelism</code>) to solve the problem that a single GPU cannot fit the whole model.</li>
<li>No need to design custom C++ code, compatible with existing Pytorch transformer implementations.</li>
<li>Able to train a GPT-2 with 8.3 billion parameters and a BERT with 3.9 billion parameters.<br><img src="/blogs/joplin_resources/fa846f2d8740035590fed9a5b6e21774.png"><br>On the above figure, model parallel means using tensor model parallelism methods proposed in this paper. Evaluation is based on weak scaling.</li>
</ol>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p><img src="/blogs/joplin_resources/ea97528d7e5fcdf8c5320fd26d4ef537.png"></p>
<p>The tensor model parallelism can be described by the above illustrations. Generally it is designed for equally partitioning transformer blocks (MLP and self-attention layers) into different parts which are stored in corresponding GPUs. Above figure uses 2 GPUs, it means both MLP and Self-Attention layer are segmented equally with two parts, with each part put in a GPU during training.</p>
<p>For MLP, we have the following equations:<br>$$ Y&#x3D; \text{GeLU}(XA) $$<br>$$ Z &#x3D; \text{Dropout}(YB) $$<br>where X and Y are activations, A and B are parameter matrices. A is split along columns such that the GeLU nonlinear function can be put in individual GPUs separately, leading to $Y_1$ and $Y_2$. Then B is split along rows giving $Z_1&#x3D;Y_1B_1$ and $Z_2&#x3D;Y_2B_2$. Before dropout, we should have $Z&#x3D;Z_1 + Z_2$, therefore, we use “all-reduce” operator to calculate the sum from different GPUs and distribute the result back to all GPUs, then dropout operator is executed in each GPU that outputs $Z$ (here I guess each GPU should share the same dropout mask).</p>
<p>In this way, the “f” function is actually a non-operation (or Identity function) and “g” is an all-reduce function in the forward pass. In the backward pass, “g” becomes Identity function and “f” becomes an all-reduce function. These two functions are the so-called conjugate functions.</p>
<p>For Self-Attention layer, we make use of the multi-head attention mechanism to do tensor model parallelism. X, again, is shared in all GPUs, while each GPU have its separate sets of attention heads where the K, Q, V are generated with its own linear projection matrices. Similarly, B is split in rows and all-reduce is applied before dropout. </p>
<p>Overall, after applying such tensor model parallelism, for each transformer layer (consisting of a attention layer and a MLP layer), there are 4 total communication operations in the forward and backward pass of a single model parallel transformer layer, i.e., four all-reduce operations involved in forward and backward passes.<br><img src="/blogs/joplin_resources/c49cabcf330bbd76d7f63d0cf30a0da4.png"></p>
<h4 id="Other-notes"><a href="#Other-notes" class="headerlink" title="Other notes"></a>Other notes</h4><ol>
<li><p>For output embedding $E_{H\times v}$ which transforms the hidden size H to vocabulary size v, we split $E$ along columns to $E_1, E_2$ and multiply with the output of the last transformer layer to get $[Y_1, Y_2] &#x3D; [XE_1, XE_2]$, then instead of using all-gather to gather $Y_1, Y_2$ to $Y&#x3D;[Y_1, Y_2]$ and distribute it to each GPU followed by cross-entropy loss (this may cause the all-gather operation to communicate $b\times s\times v$ elements in $Y$ where b is batch size and s is sequence length), they  fuse the output of $[Y_1, Y_2]$ with the cross entropy loss to reduce the dimension to $b\times s$. (Though here I don’t know how they fuse that :&lt;). </p>
</li>
<li><p>For communications between GPUs, they use NVSwitch with 300GB&#x2F;sec bandwidth for intra-server and 8 InfiniBand adapters per server with 100GB&#x2F;sec bandwidth for inter-server communications.</p>
</li>
</ol>
<h3 id="Scaling-evaluation-on-GPT-2"><a href="#Scaling-evaluation-on-GPT-2" class="headerlink" title="Scaling evaluation on GPT-2"></a>Scaling evaluation on GPT-2</h3><p><img src="/blogs/joplin_resources/798943eea9a9e4d297607659287469f7.png"><br>Here, 100% is for the baseline regarding the training throughput. Other percentages are relative to the baseline. </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2019/06/22/Quick-read-methods-of-network-compression-in-2019/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blogs/2019/06/22/Quick-read-methods-of-network-compression-in-2019/" class="post-title-link" itemprop="url">Quick read: methods of network compression in 2019</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-06-22 10:01:33" itemprop="dateCreated datePublished" datetime="2019-06-22T10:01:33+03:00">2019-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2019-07-07 12:26:40" itemprop="dateModified" datetime="2019-07-07T12:26:40+03:00">2019-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blogs/categories/Paper-reading/" itemprop="url" rel="index"><span itemprop="name">Paper reading</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Let’s quickly go through the new models related to network compression published at <em>CVPR 2019</em>, <em>ICLR 2019</em> and <em>ICML 2019</em>. Some works needs to be read and understood more carefully.</p>
<h2 id="CVPR-2019"><a href="#CVPR-2019" class="headerlink" title="CVPR 2019"></a>CVPR 2019</h2><p>CVPR is more kind of tending to solve problems in practical applications, while ICLR and ICML are more close to theoretical explanations. </p>
<h3 id="1-Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression-KSE"><a href="#1-Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression-KSE" class="headerlink" title="1. Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression[^KSE]"></a>1. Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression[^KSE]</h3><ul>
<li><p>Institutes: Xiamen University, Peng Cheng Laboratory (Shenzhen, China), Beihang University, Huawei Noahs Ark Lab, University of Buffalo and BestImage of Tencent Technology (Shanghai)</p>
</li>
<li><p>Notes</p>
</li>
</ul>
<ol>
<li>Investigate CNN compression from a novel interpretable perspective. Discover that importance of feature maps depend on sparsity and richness (using the proposed Kernel sparsity and Entropy metric);</li>
</ol>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/blogs/2019/06/22/Quick-read-methods-of-network-compression-in-2019/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2019/06/16/Going-with-small-and-fast-networks-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/avatar.gif">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blogs/2019/06/16/Going-with-small-and-fast-networks-1/" class="post-title-link" itemprop="url">Going with small and fast networks (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-06-16 16:13:53" itemprop="dateCreated datePublished" datetime="2019-06-16T16:13:53+03:00">2019-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2019-06-18 14:10:38" itemprop="dateModified" datetime="2019-06-18T14:10:38+03:00">2019-06-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blogs/categories/Paper-reading/" itemprop="url" rel="index"><span itemprop="name">Paper reading</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>In this post, we are going to look at the following neural network models: MobileNet v1[^1] &amp; v2[^2], SqueezeNet[^3], ShuffleNet v1[^4] &amp; v2[^5], NasNet[^6]. We consider the following questions:</p>
<ol>
<li><p>What in the world do they look like?</p>
</li>
<li><p>Why are they fast? Why are they small? Which one is better and Why?</p>
</li>
<li><p>Why the authors design them like that?</p>
</li>
</ol>
<p>So, let’s try to solve these doubts step by step.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/blogs/2019/06/16/Going-with-small-and-fast-networks-1/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zhuo ge ge</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blogs/js/comments.js"></script><script src="/blogs/js/utils.js"></script><script src="/blogs/js/motion.js"></script><script src="/blogs/js/sidebar.js"></script><script src="/blogs/js/next-boot.js"></script>

  






  





</body>
</html>
